#!/usr/bin/env python3
"""
GPUæ²™ç›’ç¯å¢ƒæµ‹è¯•ç¤ºä¾‹
æµ‹è¯•å„ç§GPUè®¡ç®—åœºæ™¯ï¼ŒéªŒè¯æ²™ç›’ç¯å¢ƒæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config_manager import ConfigManager
from ec2_sandbox.core import EC2SandboxEnv

def test_basic_gpu_detection():
    """æµ‹è¯•1: åŸºç¡€GPUç¯å¢ƒæ£€æµ‹"""
    print("ğŸ” æµ‹è¯•1: åŸºç¡€GPUç¯å¢ƒæ£€æµ‹")
    
    manager = ConfigManager('config.json')
    config = manager.get_sandbox_config('sandbox-gpu')
    sandbox_env = EC2SandboxEnv(config)
    sandbox = sandbox_env.create_sandbox_instance('gpu-basic-test')
    
    result = sandbox.execute_code(
        code='''
import os
import subprocess

print("=== GPUç¯å¢ƒæ£€æµ‹ ===")
print(f"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not set')}")
print(f"NVIDIA_VISIBLE_DEVICES: {os.environ.get('NVIDIA_VISIBLE_DEVICES', 'Not set')}")
print(f"PATHåŒ…å«CUDA: {'/usr/local/cuda/bin' in os.environ.get('PATH', '')}")

# æ£€æŸ¥nvidia-smi
try:
    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,driver_version', 
                           '--format=csv,noheader,nounits'], 
                          capture_output=True, text=True, timeout=10)
    if result.returncode == 0:
        gpu_info = result.stdout.strip().split(', ')
        print(f"GPUåç§°: {gpu_info[0]}")
        print(f"GPUå†…å­˜: {gpu_info[1]}MB")
        print(f"é©±åŠ¨ç‰ˆæœ¬: {gpu_info[2]}")
        print("âœ… GPUç¡¬ä»¶æ£€æµ‹æˆåŠŸ")
    else:
        print("âŒ nvidia-smiæ‰§è¡Œå¤±è´¥")
except Exception as e:
    print(f"âŒ GPUæ£€æµ‹å¼‚å¸¸: {e}")
''',
        runtime='python'
    )
    
    print(f"æ‰§è¡Œç»“æœ: {'âœ… æˆåŠŸ' if result.success else 'âŒ å¤±è´¥'}")
    if not result.success:
        print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
    return result.success

def test_pytorch_gpu():
    """æµ‹è¯•2: PyTorch GPUè®¡ç®—"""
    print("\nğŸ§  æµ‹è¯•2: PyTorch GPUæ·±åº¦å­¦ä¹ ")
    
    manager = ConfigManager('config.json')
    config = manager.get_sandbox_config('sandbox-gpu')
    sandbox_env = EC2SandboxEnv(config)
    sandbox = sandbox_env.create_sandbox_instance('pytorch-gpu-test')
    
    result = sandbox.execute_code(
        code='''
import torch
import torch.nn as nn
import torch.optim as optim
import time

print("=== PyTorch GPUæµ‹è¯• ===")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if not torch.cuda.is_available():
    print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
    exit(1)

print(f"GPUè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
print(f"å½“å‰GPU: {torch.cuda.get_device_name(0)}")

# åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 10)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# ç§»åŠ¨æ¨¡å‹åˆ°GPU
device = torch.device('cuda:0')
model = SimpleNet().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

print("âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU")

# åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
batch_size = 64
input_data = torch.randn(batch_size, 784).to(device)
target = torch.randint(0, 10, (batch_size,)).to(device)

print("âœ… æ•°æ®å·²ç§»åŠ¨åˆ°GPU")

# è®­ç»ƒå‡ ä¸ªæ­¥éª¤
print("å¼€å§‹GPUè®­ç»ƒ...")
start_time = time.time()

for epoch in range(5):
    optimizer.zero_grad()
    output = model(input_data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    
    if epoch % 2 == 0:
        print(f"Epoch {epoch}: Loss = {loss.item():.4f}")

end_time = time.time()
print(f"âœ… GPUè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

# æµ‹è¯•æ¨ç†
model.eval()
with torch.no_grad():
    test_input = torch.randn(10, 784).to(device)
    predictions = model(test_input)
    predicted_classes = torch.argmax(predictions, dim=1)
    print(f"âœ… GPUæ¨ç†å®Œæˆï¼Œé¢„æµ‹ç»“æœ: {predicted_classes.cpu().numpy()}")

print("ğŸ‰ PyTorch GPUæµ‹è¯•æˆåŠŸï¼")
''',
        runtime='python'
    )
    
    print(f"æ‰§è¡Œç»“æœ: {'âœ… æˆåŠŸ' if result.success else 'âŒ å¤±è´¥'}")
    if not result.success:
        print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
    return result.success

def test_gpu_matrix_operations():
    """æµ‹è¯•3: GPUçŸ©é˜µè¿ç®—æ€§èƒ½"""
    print("\nğŸ”¢ æµ‹è¯•3: GPUçŸ©é˜µè¿ç®—æ€§èƒ½")
    
    manager = ConfigManager('config.json')
    config = manager.get_sandbox_config('sandbox-gpu')
    sandbox_env = EC2SandboxEnv(config)
    sandbox = sandbox_env.create_sandbox_instance('matrix-gpu-test')
    
    result = sandbox.execute_code(
        code='''
import torch
import time
import numpy as np

print("=== GPUçŸ©é˜µè¿ç®—æ€§èƒ½æµ‹è¯• ===")

if not torch.cuda.is_available():
    print("âŒ CUDAä¸å¯ç”¨")
    exit(1)

# è®¾ç½®çŸ©é˜µå¤§å°
matrix_size = 2048
device = torch.device('cuda:0')

print(f"çŸ©é˜µå¤§å°: {matrix_size}x{matrix_size}")

# CPUçŸ©é˜µè¿ç®—
print("\\n--- CPUçŸ©é˜µè¿ç®— ---")
cpu_a = torch.randn(matrix_size, matrix_size)
cpu_b = torch.randn(matrix_size, matrix_size)

start_time = time.time()
cpu_result = torch.mm(cpu_a, cpu_b)
cpu_time = time.time() - start_time
print(f"CPUçŸ©é˜µä¹˜æ³•è€—æ—¶: {cpu_time:.3f}ç§’")

# GPUçŸ©é˜µè¿ç®—
print("\\n--- GPUçŸ©é˜µè¿ç®— ---")
gpu_a = cpu_a.to(device)
gpu_b = cpu_b.to(device)

# é¢„çƒ­GPU
_ = torch.mm(gpu_a, gpu_b)
torch.cuda.synchronize()

start_time = time.time()
gpu_result = torch.mm(gpu_a, gpu_b)
torch.cuda.synchronize()  # ç¡®ä¿GPUè®¡ç®—å®Œæˆ
gpu_time = time.time() - start_time

print(f"GPUçŸ©é˜µä¹˜æ³•è€—æ—¶: {gpu_time:.3f}ç§’")
print(f"GPUåŠ é€Ÿæ¯”: {cpu_time/gpu_time:.1f}x")

# éªŒè¯ç»“æœä¸€è‡´æ€§
cpu_result_check = gpu_result.cpu()
max_diff = torch.max(torch.abs(cpu_result - cpu_result_check)).item()
print(f"CPU/GPUç»“æœæœ€å¤§å·®å¼‚: {max_diff:.2e}")

if max_diff < 1e-4:
    print("âœ… CPUå’ŒGPUè®¡ç®—ç»“æœä¸€è‡´")
else:
    print("âš ï¸ CPUå’ŒGPUè®¡ç®—ç»“æœå­˜åœ¨å·®å¼‚")

# GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
if hasattr(torch.cuda, 'memory_allocated'):
    memory_allocated = torch.cuda.memory_allocated(device) / 1024**2
    memory_reserved = torch.cuda.memory_reserved(device) / 1024**2
    print(f"\\nGPUå†…å­˜ä½¿ç”¨: {memory_allocated:.1f}MB (å·²åˆ†é…)")
    print(f"GPUå†…å­˜ä¿ç•™: {memory_reserved:.1f}MB (å·²ä¿ç•™)")

print("ğŸ‰ GPUçŸ©é˜µè¿ç®—æµ‹è¯•æˆåŠŸï¼")
''',
        runtime='python'
    )
    
    print(f"æ‰§è¡Œç»“æœ: {'âœ… æˆåŠŸ' if result.success else 'âŒ å¤±è´¥'}")
    if not result.success:
        print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
    return result.success

def test_gpu_image_processing():
    """æµ‹è¯•4: GPUå›¾åƒå¤„ç†"""
    print("\nğŸ–¼ï¸ æµ‹è¯•4: GPUå›¾åƒå¤„ç†")
    
    manager = ConfigManager('config.json')
    config = manager.get_sandbox_config('sandbox-gpu')
    sandbox_env = EC2SandboxEnv(config)
    sandbox = sandbox_env.create_sandbox_instance('image-gpu-test')
    
    result = sandbox.execute_code(
        code='''
import torch
import torch.nn.functional as F
import time

print("=== GPUå›¾åƒå¤„ç†æµ‹è¯• ===")

if not torch.cuda.is_available():
    print("âŒ CUDAä¸å¯ç”¨")
    exit(1)

device = torch.device('cuda:0')

# åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒæ•°æ® (batch_size, channels, height, width)
batch_size = 32
channels = 3
height, width = 224, 224

print(f"å›¾åƒæ‰¹æ¬¡: {batch_size}å¼ ")
print(f"å›¾åƒå°ºå¯¸: {channels}x{height}x{width}")

# ç”Ÿæˆéšæœºå›¾åƒæ•°æ®
images = torch.randn(batch_size, channels, height, width).to(device)
print("âœ… å›¾åƒæ•°æ®å·²ç§»åŠ¨åˆ°GPU")

# å›¾åƒå¤„ç†æ“ä½œ
print("\\n--- GPUå›¾åƒå¤„ç†æ“ä½œ ---")

start_time = time.time()

# 1. å›¾åƒç¼©æ”¾
resized = F.interpolate(images, size=(112, 112), mode='bilinear', align_corners=False)
print(f"âœ… å›¾åƒç¼©æ”¾: {images.shape} -> {resized.shape}")

# 2. å›¾åƒå·ç§¯ (æ¨¡æ‹Ÿè¾¹ç¼˜æ£€æµ‹)
sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).to(device)
sobel_x = sobel_x.view(1, 1, 3, 3).repeat(channels, 1, 1, 1)

edges = F.conv2d(images, sobel_x, padding=1, groups=channels)
print(f"âœ… è¾¹ç¼˜æ£€æµ‹å·ç§¯: {edges.shape}")

# 3. å›¾åƒæ± åŒ–
pooled = F.max_pool2d(images, kernel_size=2, stride=2)
print(f"âœ… æœ€å¤§æ± åŒ–: {images.shape} -> {pooled.shape}")

# 4. å›¾åƒå½’ä¸€åŒ–
normalized = F.normalize(images, p=2, dim=1)
print(f"âœ… å›¾åƒå½’ä¸€åŒ–: {normalized.shape}")

# 5. æ‰¹é‡å›¾åƒå˜æ¢
flipped = torch.flip(images, dims=[3])  # æ°´å¹³ç¿»è½¬
rotated = torch.rot90(images, k=1, dims=[2, 3])  # æ—‹è½¬90åº¦
print(f"âœ… å›¾åƒå˜æ¢: ç¿»è½¬å’Œæ—‹è½¬")

torch.cuda.synchronize()
processing_time = time.time() - start_time

print(f"\\nGPUå›¾åƒå¤„ç†æ€»è€—æ—¶: {processing_time:.3f}ç§’")
print(f"å¹³å‡æ¯å¼ å›¾åƒ: {processing_time/batch_size*1000:.1f}æ¯«ç§’")

# å†…å­˜ä½¿ç”¨æƒ…å†µ
memory_used = torch.cuda.memory_allocated(device) / 1024**2
print(f"GPUå†…å­˜ä½¿ç”¨: {memory_used:.1f}MB")

print("ğŸ‰ GPUå›¾åƒå¤„ç†æµ‹è¯•æˆåŠŸï¼")
''',
        runtime='python'
    )
    
    print(f"æ‰§è¡Œç»“æœ: {'âœ… æˆåŠŸ' if result.success else 'âŒ å¤±è´¥'}")
    if not result.success:
        print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
    return result.success

def test_gpu_memory_management():
    """æµ‹è¯•5: GPUå†…å­˜ç®¡ç†"""
    print("\nğŸ’¾ æµ‹è¯•5: GPUå†…å­˜ç®¡ç†")
    
    manager = ConfigManager('config.json')
    config = manager.get_sandbox_config('sandbox-gpu')
    sandbox_env = EC2SandboxEnv(config)
    sandbox = sandbox_env.create_sandbox_instance('memory-gpu-test')
    
    result = sandbox.execute_code(
        code='''
import torch
import gc

print("=== GPUå†…å­˜ç®¡ç†æµ‹è¯• ===")

if not torch.cuda.is_available():
    print("âŒ CUDAä¸å¯ç”¨")
    exit(1)

device = torch.device('cuda:0')

def get_gpu_memory():
    """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    allocated = torch.cuda.memory_allocated(device) / 1024**2
    reserved = torch.cuda.memory_reserved(device) / 1024**2
    return allocated, reserved

# åˆå§‹å†…å­˜çŠ¶æ€
torch.cuda.empty_cache()
initial_allocated, initial_reserved = get_gpu_memory()
print(f"åˆå§‹å†…å­˜ - å·²åˆ†é…: {initial_allocated:.1f}MB, å·²ä¿ç•™: {initial_reserved:.1f}MB")

# åˆ†é…å¤§é‡GPUå†…å­˜
print("\\n--- åˆ†é…GPUå†…å­˜ ---")
tensors = []
for i in range(5):
    size = 1024 * (i + 1)  # é€æ¸å¢å¤§
    tensor = torch.randn(size, size).to(device)
    tensors.append(tensor)
    
    allocated, reserved = get_gpu_memory()
    print(f"åˆ†é… {size}x{size} å¼ é‡ - å·²åˆ†é…: {allocated:.1f}MB, å·²ä¿ç•™: {reserved:.1f}MB")

max_allocated, max_reserved = get_gpu_memory()
print(f"\\næœ€å¤§å†…å­˜ä½¿ç”¨ - å·²åˆ†é…: {max_allocated:.1f}MB, å·²ä¿ç•™: {max_reserved:.1f}MB")

# é‡Šæ”¾éƒ¨åˆ†å†…å­˜
print("\\n--- é‡Šæ”¾GPUå†…å­˜ ---")
del tensors[2:4]  # åˆ é™¤ä¸­é—´ä¸¤ä¸ªå¼ é‡
gc.collect()
torch.cuda.empty_cache()

after_partial_free = get_gpu_memory()
print(f"éƒ¨åˆ†é‡Šæ”¾å - å·²åˆ†é…: {after_partial_free[0]:.1f}MB, å·²ä¿ç•™: {after_partial_free[1]:.1f}MB")

# é‡Šæ”¾æ‰€æœ‰å†…å­˜
del tensors
gc.collect()
torch.cuda.empty_cache()

final_allocated, final_reserved = get_gpu_memory()
print(f"å®Œå…¨é‡Šæ”¾å - å·²åˆ†é…: {final_allocated:.1f}MB, å·²ä¿ç•™: {final_reserved:.1f}MB")

# å†…å­˜ç®¡ç†éªŒè¯
memory_freed = max_allocated - final_allocated
print(f"\\nå†…å­˜é‡Šæ”¾é‡: {memory_freed:.1f}MB")

if final_allocated <= initial_allocated + 10:  # å…è®¸10MBè¯¯å·®
    print("âœ… GPUå†…å­˜ç®¡ç†æ­£å¸¸")
else:
    print("âš ï¸ å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")

# GPUè®¾å¤‡ä¿¡æ¯
props = torch.cuda.get_device_properties(device)
print(f"\\nGPUè®¾å¤‡ä¿¡æ¯:")
print(f"  åç§°: {props.name}")
print(f"  æ€»å†…å­˜: {props.total_memory / 1024**3:.1f}GB")
print(f"  å¤šå¤„ç†å™¨æ•°é‡: {props.multi_processor_count}")
print(f"  CUDAè®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")

print("ğŸ‰ GPUå†…å­˜ç®¡ç†æµ‹è¯•æˆåŠŸï¼")
''',
        runtime='python'
    )
    
    print(f"æ‰§è¡Œç»“æœ: {'âœ… æˆåŠŸ' if result.success else 'âŒ å¤±è´¥'}")
    if not result.success:
        print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
    return result.success

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 70)
    print("ğŸš€ GPUæ²™ç›’ç¯å¢ƒç»¼åˆæµ‹è¯•")
    print("=" * 70)
    
    tests = [
        ("åŸºç¡€GPUç¯å¢ƒæ£€æµ‹", test_basic_gpu_detection),
        ("PyTorch GPUæ·±åº¦å­¦ä¹ ", test_pytorch_gpu),
        ("GPUçŸ©é˜µè¿ç®—æ€§èƒ½", test_gpu_matrix_operations),
        ("GPUå›¾åƒå¤„ç†", test_gpu_image_processing),
        ("GPUå†…å­˜ç®¡ç†", test_gpu_memory_management)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            print(f"\n{'='*50}")
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"âŒ {test_name}æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æµ‹è¯•ç»“æœæ±‡æ€»
    print("\n" + "=" * 70)
    print("ğŸ“Š GPUæ²™ç›’ç¯å¢ƒæµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 70)
    
    passed = 0
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:20} : {status}")
        if success:
            passed += 1
    
    total = len(results)
    success_rate = (passed / total) * 100
    
    print("-" * 70)
    print(f"æµ‹è¯•é€šè¿‡ç‡: {passed}/{total} ({success_rate:.1f}%)")
    
    if success_rate >= 80:
        print("\nğŸ‰ æ­å–œï¼GPUæ²™ç›’ç¯å¢ƒè¿è¡Œæ­£å¸¸ï¼")
        print("ğŸ’¡ ç¯å¢ƒå·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥è¿›è¡ŒGPUåŠ é€Ÿçš„æœºå™¨å­¦ä¹ å’Œç§‘å­¦è®¡ç®—ä»»åŠ¡")
    elif success_rate >= 60:
        print("\nâš ï¸ GPUæ²™ç›’ç¯å¢ƒåŸºæœ¬å¯ç”¨ï¼Œä½†å­˜åœ¨éƒ¨åˆ†é—®é¢˜")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•é¡¹ç›®")
    else:
        print("\nâŒ GPUæ²™ç›’ç¯å¢ƒå­˜åœ¨ä¸¥é‡é—®é¢˜")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥GPUé©±åŠ¨ã€CUDAå®‰è£…å’ŒPyTorché…ç½®")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
